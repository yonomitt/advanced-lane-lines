{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "- Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "- Apply a distortion correction to raw images.\n",
    "- Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "- Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "- Detect lane pixels and fit to find the lane boundary.\n",
    "- Determine the curvature of the lane and vehicle position with respect to center.\n",
    "- Warp the detected lane boundaries back onto the original image.\n",
    "- Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Imports and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANE_WIDTH_METERS = 3.7\n",
    "LANE_LENGTH_METERS = 30.0\n",
    "\n",
    "Point = namedtuple('Point', ['x', 'y'])\n",
    "Size = namedtuple('Size', ['width', 'height'])\n",
    "\n",
    "def np_zip(a, b):\n",
    "    \"\"\"A version of zip for numpy arrays\"\"\"\n",
    "    \n",
    "    return np.vstack([a, b]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Camera Calibration\n",
    "\n",
    "Camera calibration only needs to occur once, but I have put it in a function to encourage good coding behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_checkerboard_corners(img, nx, ny):\n",
    "    \"\"\"Helper function to detect the internal corners of an image of a checkerboard\n",
    "    \n",
    "    Parameters:\n",
    "        • img - image of a checkerboard\n",
    "        • nx - number of internal checkerboard corners in the x direction\n",
    "        • ny - number of internal checkerboard corners in the y direction\n",
    "        \n",
    "    Returns:\n",
    "        A list of corners\"\"\"\n",
    "    \n",
    "    # openCV reads in images BGR (instead of RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chess board corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "    \n",
    "    if ret:\n",
    "        return corners\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "def camera_calibration(img_files, nx=9, ny=6):\n",
    "    \"\"\"Calibrate the camera based on several checkerboard images taken.\n",
    "    \n",
    "    Parameters:\n",
    "        • img_files - list of checkboard image files, ideally taken from different angles and distances\n",
    "        • nx - number of internal checkerboard corners in the x direction for all images\n",
    "        • ny - number of internal checkerboard corners in the y direction for all images\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of the camera matrix and the distortion coefficients\"\"\"\n",
    "    \n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((ny * nx, 3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1,2)\n",
    "    \n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d point in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "    for idx, img_file in enumerate(img_files):\n",
    "        img = cv2.imread(img_file)\n",
    "        \n",
    "        # find the checkerboard corners\n",
    "        corners = find_checkerboard_corners(img, nx, ny)\n",
    "\n",
    "        # If found, add object points, image points (after refining them)\n",
    "        if len(corners):\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "    \n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    \n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size, None, None)\n",
    "    \n",
    "    return (mtx, dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_files = glob.glob('camera_cal/*.jpg')\n",
    "mtx, dist = camera_calibration(img_files)\n",
    "\n",
    "img_file = img_files[13]\n",
    "\n",
    "img = cv2.imread(img_file)\n",
    "cv2.imwrite('output_images/checkerboard-00-orig.jpg', img)\n",
    "\n",
    "plt.title('Original Image of a Checkerboard')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "corners = find_checkerboard_corners(img, 9, 6)\n",
    "cv2.drawChessboardCorners(img, (9, 6), corners, True)\n",
    "cv2.imwrite('output_images/checkerboard-01-corners.jpg', img)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title('Detected Corners on a Checkerboard')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Undistort\n",
    "\n",
    "This will need to be run on all images produced by the camera that was calibrated in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def undistort(img, mtx, dist):\n",
    "    \"\"\"Undistort an image using the camera matrix and the distorition coefficients returned by the 'camera_calibration'\n",
    "    function\n",
    "    \n",
    "    Parameters:\n",
    "        • img - image that needs to be undistorted\n",
    "        • mtx - camera matrix\n",
    "        • dist - distortion coefficients\n",
    "        \n",
    "    Returns:\n",
    "        An undistorted image\"\"\"\n",
    "    \n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(img_files[13])\n",
    "\n",
    "plt.title('Original Image of a Checkerboard')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "img = undistort(img, mtx, dist)\n",
    "cv2.imwrite('output_images/checkerboard-02-undistorted.jpg', img)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Undistorted Image\")\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "undistort_imgs = []\n",
    "file_bases = [f.split('/')[-1][:-4] for f in glob.glob('test_images/*.jpg')]\n",
    "\n",
    "for filename, file_base in zip(glob.glob('test_images/*.jpg'), file_bases):\n",
    "    \n",
    "    orig_img = cv2.imread(filename)\n",
    "    cv2.imwrite('output_images/{}-00-orig.jpg'.format(file_base), orig_img)\n",
    "    \n",
    "    undistort_img = undistort(orig_img, mtx, dist)\n",
    "    cv2.imwrite('output_images/{}-01-undist.jpg'.format(file_base), undistort_img)\n",
    "\n",
    "    undistort_imgs.append(undistort_img)\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.set_title('Original Image')\n",
    "    ax1.imshow(cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    ax2.set_title('Undistorted Image')\n",
    "    ax2.imshow(cv2.cvtColor(undistort_img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Thresholding functions\n",
    "\n",
    "Various thresholding functions determine lane boundries. To make the algorithm more robust, these will be combined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def absolute_sobel_threshold(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "\n",
    "    \"\"\"Creates a binary image based on the gradient ine one direction of the input image.\n",
    "    \n",
    "    This function uses the Sobel operator to calculate the derivative.\n",
    "    \n",
    "    Parameters:\n",
    "        • img - input image\n",
    "        • orient - direction to take the derivative ('x' or 'y')\n",
    "        • sobel_kernel - an odd number to define the size of the Sobel kernel\n",
    "        • thresh - tuple of low and high thresholds to be included in the output binary\n",
    "        \n",
    "    Returns:\n",
    "        A single channel binary image of detected edges in the original image\"\"\"\n",
    "    \n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2) Take the derivative in x or y given orient = 'x' or 'y'\n",
    "    filter_type = (orient == 'x', orient == 'y')\n",
    "    sobel = cv2.Sobel(gray, cv2.CV_64F, *filter_type, ksize=sobel_kernel)\n",
    "\n",
    "    # 3) Take the absolute value of the derivative or gradient\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "\n",
    "    # 4) Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255 * abs_sobel / np.max(abs_sobel))\n",
    "\n",
    "    # 5) Create a mask of 1's where the scaled gradient meets the thresholds \n",
    "    grad_binary = np.zeros_like(scaled_sobel)\n",
    "    grad_binary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    return grad_binary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magnitude_threshold(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "\n",
    "    \"\"\"Creates a binary image based on the overal magnitude of the gradient of the input image.\n",
    "    \n",
    "    This function uses the Sobel operator to calculate the derivative.\n",
    "    \n",
    "    Parameters:\n",
    "        • img - input image\n",
    "        • sobel_kernel - an odd number to define the size of the Sobel kernel\n",
    "        • thresh - tuple of low and high thresholds to be included in the output binary\n",
    "        \n",
    "    Returns:\n",
    "        A single channel binary image of detected edges in the original image\"\"\"\n",
    "\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "\n",
    "    # 3) Calculate the magnitude \n",
    "    abs_sobelxy = (sobelx ** 2 + sobely ** 2) ** 0.5\n",
    "\n",
    "    # 4) Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255 * abs_sobelxy / np.max(abs_sobelxy))\n",
    "\n",
    "    # 5) Create a binary mask where mag thresholds are met\n",
    "    mag_binary = np.zeros_like(scaled_sobel)\n",
    "    mag_binary[(scaled_sobel >= mag_thresh[0]) & (scaled_sobel <= mag_thresh[1])] = 1\n",
    "\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    return mag_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def direction_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "\n",
    "    \"\"\"Creates a binary image based on the gradient direction of the input image.\n",
    "    \n",
    "    This function uses the Sobel operator to calculate the derivative.\n",
    "    \n",
    "    Parameters:\n",
    "        • img - input image\n",
    "        • sobel_kernel - an odd number to define the size of the Sobel kernel\n",
    "        • thresh - tuple of low and high thresholds to be included in the output binary\n",
    "        \n",
    "    Returns:\n",
    "        A single channel binary image of detected edges in the original image\"\"\"\n",
    "\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "\n",
    "    # 3) Take the absolute value of the x and y gradients\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "\n",
    "    # 4) Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient \n",
    "    direction = np.arctan2(abs_sobely, abs_sobelx)\n",
    "\n",
    "    # 5) Create a binary mask where direction thresholds are met\n",
    "    dir_binary = np.zeros_like(direction, dtype=img.dtype)\n",
    "    dir_binary[(direction >= thresh[0]) & (direction <= thresh[1])] = 1\n",
    "\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    return dir_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saturation_threshold(img, thresh=(0, 255)):\n",
    "    \n",
    "    \"\"\"Creates a binary image based on the saturation channel of the input image converted to HLS color space.\n",
    "        \n",
    "    Parameters:\n",
    "        • img - input image\n",
    "        • thresh - tuple of low and high thresholds to be included in the output binary\n",
    "        \n",
    "    Returns:\n",
    "        A thresholded single channel binary image of the original image\"\"\"\n",
    "\n",
    "    # 1) Convert to HLS color space\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "\n",
    "    # 2) Apply a threshold to the S channel\n",
    "    s = hls[:, :, 2]\n",
    "    sat_binary = np.zeros_like(s)\n",
    "    sat_binary[(s > thresh[0]) & (s <= thresh[1])] = 1\n",
    "\n",
    "    # 3) Return a binary image of threshold result\n",
    "    return sat_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def region_of_interest(img):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    vertices = np.array([[[0, 720], [588, 446], [692, 446], [1280, 720]]], dtype=np.int32)\n",
    "    \n",
    "    # defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)\n",
    "    mask = np.dstack((mask, mask, mask))\n",
    "\n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(mask.shape) > 2:\n",
    "        channel_count = mask.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        if channel_count > 1:\n",
    "            ignore_mask_color = (255,) * channel_count\n",
    "        else:\n",
    "            ignore_mask_color = 255\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "\n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "\n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask[:, :, 0])\n",
    "    return masked_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def threshold(img):\n",
    "    \n",
    "    \"\"\"Creates a binary image based on combining various thresholding techniques.\n",
    "    \n",
    "    Paramters:\n",
    "        • img - input image\n",
    "        \n",
    "    Returns:\n",
    "        A binary image where the lane markers are clearly visible\"\"\"\n",
    "    \n",
    "    ksize = 5\n",
    "    \n",
    "    gradx = absolute_sobel_threshold(img, orient='x', sobel_kernel=ksize, thresh=(20, 100))\n",
    "    grady = absolute_sobel_threshold(img, orient='y', sobel_kernel=ksize, thresh=(20, 100))\n",
    "    mag_binary = magnitude_threshold(img, sobel_kernel=ksize, mag_thresh=(20, 100))\n",
    "    dir_binary = direction_threshold(img, sobel_kernel=ksize, thresh=(0.7, 1.3))\n",
    "    \n",
    "    sat_binary = saturation_threshold(img, thresh=(90, 255))\n",
    "\n",
    "    combined = np.zeros_like(dir_binary)\n",
    "    combined[(((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))) | (sat_binary == 1)] = 1\n",
    "\n",
    "    return region_of_interest(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresholded_imgs = []\n",
    "\n",
    "for undistort_img, file_base in zip(undistort_imgs, file_bases):\n",
    "\n",
    "    thresholded_img = threshold(undistort_img)\n",
    "    cv2.imwrite('output_images/{}-02-thresh.jpg'.format(file_base), np.dstack((thresholded_img, thresholded_img, thresholded_img)) * 255)\n",
    "    \n",
    "    thresholded_imgs.append(thresholded_img)\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    ax1.set_title('Undistorted Image')\n",
    "    ax1.imshow(cv2.cvtColor(undistort_img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    ax2.set_title('Thresholded Image')\n",
    "    ax2.imshow(thresholded_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Perspective Transformation\n",
    "\n",
    "These functions will transform images from perspective view to birds-eye view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transform_matrices():\n",
    "    \n",
    "    \"\"\"Generates the transform matrix and the inverse transform matrix to warp an image from\n",
    "    perspective view to birds-eye view\n",
    "    \n",
    "    Returns:\n",
    "        A tuple of the transform matrix and the inverse transform matrix\"\"\"\n",
    "    \n",
    "    # define the source and destination points for the desired transformation\n",
    "    src = np.float32([[140, 720], [588, 446], [692, 446], [1140, 720]])    \n",
    "    dst = np.float32([[140, 720], [140, 0], [1140, 0], [1140, 720]])\n",
    "    \n",
    "    # get the transform matrix\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    \n",
    "    # get the inverse transform matrix\n",
    "    M_inv = cv2.getPerspectiveTransform(dst, src)\n",
    "    \n",
    "    return (M, M_inv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_birdseye(img):\n",
    "    \n",
    "    \"\"\"Applies a transformation to warp an image. The result is a birds-eye view of the lane.\n",
    "    \n",
    "    Parameters:\n",
    "        • img - input image of the lane\n",
    "        \n",
    "    Returns:\n",
    "        A birds-eye view image of the lane\"\"\"\n",
    "\n",
    "    # get the transform matrices\n",
    "    M, M_inv = gen_transform_matrices()\n",
    "    \n",
    "    # get the image size\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    \n",
    "    # warp the image\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "birdseye_imgs = []\n",
    "\n",
    "for thresholded_img, file_base in zip(thresholded_imgs, file_bases):\n",
    "\n",
    "    birdseye_img = transform_birdseye(thresholded_img)\n",
    "    cv2.imwrite('output_images/{}-03-warp.jpg'.format(file_base), np.dstack((birdseye_img, birdseye_img, birdseye_img)) * 255)\n",
    "    \n",
    "    birdseye_imgs.append(birdseye_img)\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.set_title('Thresholded Image')\n",
    "    ax1.imshow(thresholded_img, cmap='gray')\n",
    "\n",
    "    ax2.set_title('Birds-Eye Warped Image')\n",
    "    ax2.imshow(birdseye_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Detect Lane Markings\n",
    "\n",
    "Detect the pixels associated with the lane markings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist = np.sum(birdseye_img[np.int(birdseye_img.shape[0]/2):,:], axis=0)\n",
    "center_x = np.int(hist.shape[0] / 2)\n",
    "left_peak = np.where(hist == np.max(hist[:center_x]))[0][0]\n",
    "right_peak = np.where(hist[center_x:] == np.max(hist[center_x:]))[0][0] + center_x\n",
    "\n",
    "print(\"{} - {} - {}\".format(left_peak, center_x, right_peak))\n",
    "plt.plot(hist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_lane_markings(img):\n",
    "    \n",
    "    \"\"\"Detect the lane markings by using peaks in a histogram to guess at the 'x' location of the lane markings\n",
    "    and a sliding window starting at the previously found 'x' position as it moves in the 'y' direction\n",
    "    \n",
    "    Parameter:\n",
    "        • img - binary birds-eye view image of the lane\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of:\n",
    "            • list of left lane pixels\n",
    "            • list of right lane pixels\n",
    "            • list of left windows\n",
    "            • list of right windows\"\"\"\n",
    "    \n",
    "    # calculate a histogram of the lower half of the image. This should help us determine the starting x position\n",
    "    # of the two lane markings\n",
    "    hist = np.sum(img[np.int(img.shape[0] / 2):,:], axis=0)\n",
    "    \n",
    "    # find the center of the image and the x coordinate of the left and right peaks in the histogram. These should\n",
    "    # hopefully correspond to the left and right lane markings\n",
    "    center_x = np.int(hist.shape[0] / 2)\n",
    "    left_x = np.where(hist == np.max(hist[:center_x]))[0][0]\n",
    "    right_x = np.where(hist[center_x:] == np.max(hist[center_x:]))[0][0] + center_x\n",
    "    \n",
    "    nwindows = 9\n",
    "    window_width = 200\n",
    "    \n",
    "    half_window_size = Size(np.int(window_width / 2), np.int(img.shape[0] / nwindows))\n",
    "    \n",
    "    # minimum number of pixels found to recenter window\n",
    "    min_pixels = 50\n",
    "    \n",
    "    # identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = img.nonzero()\n",
    "    nonzero_y = np.array(nonzero[0])\n",
    "    nonzero_x = np.array(nonzero[1])\n",
    "\n",
    "    all_left_idxs = []\n",
    "    all_right_idxs = []\n",
    "    \n",
    "    left_windows = []\n",
    "    right_windows = []\n",
    "\n",
    "    for i in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        min_y = max(img.shape[0] - (i + 1) * half_window_size.height, 0)\n",
    "        max_y = img.shape[0] - i * half_window_size.height\n",
    "\n",
    "        min_x_left = max(left_x - half_window_size.width, 0)\n",
    "        max_x_left = min(left_x + half_window_size.width, img.shape[1])\n",
    "        \n",
    "        min_x_right = max(right_x - half_window_size.width, 0)\n",
    "        max_x_right = min(right_x + half_window_size.width, img.shape[1])\n",
    "\n",
    "        left_windows.append([(min_x_left, min_y), (max_x_left, max_y)])\n",
    "        right_windows.append([(min_x_right, min_y), (max_x_right, max_y)])\n",
    "        \n",
    "        # identify the nonzero pixels in x and y within the window\n",
    "        left_idxs = ((nonzero_y >= min_y) & (nonzero_y < max_y) & (nonzero_x >= min_x_left) & (nonzero_x < max_x_left)).nonzero()[0]\n",
    "        right_idxs = ((nonzero_y >= min_y) & (nonzero_y < max_y) & (nonzero_x >= min_x_right) & (nonzero_x < max_x_right)).nonzero()[0]\n",
    "\n",
    "        # append these indices to the lists\n",
    "        all_left_idxs.append(left_idxs)\n",
    "        all_right_idxs.append(right_idxs)\n",
    "\n",
    "        # if you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(left_idxs) > min_pixels:\n",
    "            left_x = np.int(np.mean(nonzero_x[left_idxs]))\n",
    "        if len(right_idxs) > min_pixels:        \n",
    "            right_x = np.int(np.mean(nonzero_x[right_idxs]))\n",
    "\n",
    "    # concatenate the arrays of indices\n",
    "    all_left_idxs = np.concatenate(all_left_idxs)\n",
    "    all_right_idxs = np.concatenate(all_right_idxs)\n",
    "\n",
    "    # extract left and right line pixel positions\n",
    "    left_xs = nonzero_x[all_left_idxs]\n",
    "    left_ys = nonzero_y[all_left_idxs] \n",
    "    right_xs = nonzero_x[all_right_idxs]\n",
    "    right_ys = nonzero_y[all_right_idxs] \n",
    "\n",
    "    return (np_zip(left_ys, left_xs), np_zip(right_ys, right_xs), left_windows, right_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_curve(points, order=2):\n",
    "\n",
    "    \"\"\"Fits a curve to the points\n",
    "    \n",
    "    Parameters:\n",
    "        • points - list of points\n",
    "        • order - order of the polynomial to fit\n",
    "        \n",
    "    Returns:\n",
    "        Parameters describing a fitted curve of the specified order\"\"\"\n",
    "    \n",
    "    return np.polyfit(points[:, 0], points[:, 1], order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_windows(img, left, right, color=[0, 255, 0], thickness=2):\n",
    "\n",
    "    \"\"\"Draws the windows used on the image\n",
    "    \n",
    "    Pameters:\n",
    "        • img - input three channel birds-eye view image\n",
    "        • left - list of windows used to discover the left lane\n",
    "        • right - list of windows used to discover the right lane\n",
    "        • color - tuple of B, G, R values\n",
    "        • thickness - thickness of the lines\n",
    "        \n",
    "    Returns:\n",
    "        A three channel image with the windows overlaying\"\"\"\n",
    "    \n",
    "    out = np.copy(img)\n",
    "    \n",
    "    for l, r in zip(left, right):\n",
    "        cv2.rectangle(out, l[0], l[1], color, thickness)        \n",
    "        cv2.rectangle(out, r[0], r[1], color, thickness)\n",
    "        \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_curve_points(img_shape, poly):\n",
    "    \n",
    "    \"\"\"Generates a list of x and y points for a curve defined by the curve coefficients\n",
    "    \n",
    "    Parameters:\n",
    "        • img_shape - shape of the image used\n",
    "        • poly - coefficients describing the polynomial\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of lists of the x and y coordinates for the curve based on the image shape\"\"\"\n",
    "    \n",
    "    # y values to plot\n",
    "    plot_y = np.arange(img.shape[0])\n",
    "    \n",
    "    # polynomial powers\n",
    "    powers = np.arange(len(poly) - 1, -1, -1).reshape(1, len(poly))\n",
    "    \n",
    "    # calculate x position\n",
    "    plot_x = np.sum((plot_y.reshape(img.shape[0], 1) ** powers) * poly, axis=1).astype(np.int)\n",
    "    \n",
    "    return (plot_x, plot_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_fitted_curves(img, left, right, color=[0, 255, 255], thickness=5):\n",
    "    \n",
    "    \"\"\"Draws the fitted curves for the lanes on the image\n",
    "    \n",
    "    Parameters:\n",
    "        • img - input three channel birds-eye view image\n",
    "        • left - parameters that describe the curve for the left lane\n",
    "        • right - parameters that describe the curve for the right lane\n",
    "        • color - tuple of B, G, R values\n",
    "        \n",
    "    Returns:\n",
    "        A three channel image with the best fit curve for the lanes\"\"\"\n",
    "    \n",
    "    out = np.copy(img)\n",
    "    \n",
    "    for poly in (left, right):\n",
    "\n",
    "        plot_x, plot_y = gen_curve_points(img.shape, poly)\n",
    "\n",
    "        try:\n",
    "            offsets_x = np.arange(thickness) - np.ceil(thickness / 2 - 1)\n",
    "            for i in offsets_x.astype(np.int):\n",
    "                out[plot_y, plot_x + i] = color\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detected_imgs = []\n",
    "\n",
    "for birdseye_img, file_base in zip(birdseye_imgs, file_bases):\n",
    "\n",
    "    left_points, right_points, left_windows, right_windows = detect_lane_markings(birdseye_img)\n",
    "\n",
    "    left_poly = fit_curve(left_points)\n",
    "    right_poly = fit_curve(right_points)\n",
    "    \n",
    "    detected_img = np.dstack((birdseye_img, birdseye_img, birdseye_img)) * 255\n",
    "    \n",
    "    detected_img[left_points[:, 0], left_points[:, 1]] = [0, 0, 255]\n",
    "    detected_img[right_points[:, 0], right_points[:, 1]] = [255, 0, 0]\n",
    "    \n",
    "    detected_img = draw_windows(detected_img, left_windows, right_windows)\n",
    "    detected_img = draw_fitted_curves(detected_img, left_poly, right_poly)\n",
    "    \n",
    "    cv2.imwrite('output_images/{}-04-detect.jpg'.format(file_base), detected_img)\n",
    "    \n",
    "    detected_imgs.append(detected_img)\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    ax1.set_title('Birds-Eye Warped Image')\n",
    "    ax1.imshow(birdseye_img, cmap='gray')\n",
    "\n",
    "    ax2.set_title('Detected Lanes')\n",
    "    ax2.imshow(cv2.cvtColor(detected_img, cv2.COLOR_BGR2RGB))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Calculate Curvature and Position\n",
    "\n",
    "Calculate the curvature of the road and the position of the car in the lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_lane_curvature(img_shape, left_poly, right_poly):\n",
    "    \n",
    "    \"\"\"Calculate the curvature of the lane\n",
    "    \n",
    "    Parameters:\n",
    "        • img_shape - shape of the image\n",
    "        • left_poly - coefficients describing the polynomial of the left lane marker\n",
    "        • right_poly - coefficients describing the polynomial of the right lane marker\n",
    "\n",
    "    Returns:\n",
    "        The radius of the curvature of the lane in meters\"\"\"\n",
    "    \n",
    "    # generate the points of the best fit lane marking curves\n",
    "    left_x, left_y = gen_curve_points(img_shape, left_poly)\n",
    "    right_x, right_y = gen_curve_points(img_shape, right_poly)\n",
    "\n",
    "    # calculate the width of the lane in pixels\n",
    "    lane_width_pixels = right_x[-1] - left_x[-1]\n",
    "    \n",
    "    # point to evaluate the curvature\n",
    "    y_eval = img_shape[0] - 1\n",
    "    \n",
    "    # define conversions in x and y from pixels space to meters\n",
    "    mpp = Point(LANE_WIDTH_METERS / lane_width_pixels, LANE_LENGTH_METERS / img_shape[0])\n",
    "\n",
    "    # calculate the new radii of curvature\n",
    "    radii = []\n",
    "    \n",
    "    for poly in (left_poly, right_poly):\n",
    "        radius = ((1 + (2 * poly[0] * y_eval * mpp.y + poly[1]) ** 2) ** 1.5) / np.absolute(2 * poly[0])\n",
    "        radii.append(radius)\n",
    "        \n",
    "    return sum(radii) / len(radii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_lane_center_offset(img_shape, left_poly, right_poly):\n",
    "    \n",
    "    \"\"\"Calculate how much the car is offset from the center of the lane, assuming the camera\n",
    "    is at the center of the car\n",
    "    \n",
    "    Parameters:\n",
    "        • img_shape - shape of the image\n",
    "        • left_poly - coefficients describing the polynomial of the left lane marker\n",
    "        • right_poly - coefficients describing the polynomial of the right lane marker\n",
    "\n",
    "    Returns:\n",
    "        Offset from the center of the lane in meters\"\"\"\n",
    "    \n",
    "    # generate the points of the best fit lane marking curves\n",
    "    left_x, left_y = gen_curve_points(img_shape, left_poly)\n",
    "    right_x, right_y = gen_curve_points(img_shape, right_poly)\n",
    "\n",
    "    # calculate the width of the lane in pixels\n",
    "    lane_width_pixels = right_x[-1] - left_x[-1]\n",
    "\n",
    "    # calculate meters per pixels\n",
    "    meters_per_pixel = LANE_WIDTH_METERS / lane_width_pixels\n",
    "    \n",
    "    # calculate the offset in pixels\n",
    "    offset_pixels = img_shape[0] / 2 - lane_width_pixels / 2 + left_x[-1]\n",
    "    \n",
    "    # convert the offset to meters\n",
    "    offset_meters = offset_pixels * meters_per_pixel\n",
    "    \n",
    "    return offset_meters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positionals = []\n",
    "\n",
    "for birdseye_img, file_base in zip(birdseye_imgs, file_bases):\n",
    "\n",
    "    left_points, right_points, left_windows, right_windows = detect_lane_markings(birdseye_img)\n",
    "\n",
    "    left_poly = fit_curve(left_points)\n",
    "    right_poly = fit_curve(right_points)\n",
    "        \n",
    "    radius = calc_lane_curvature(birdseye_img.shape, left_poly, right_poly)\n",
    "    offset = calc_lane_center_offset(birdseye_img.shape, left_poly, right_poly)\n",
    "    \n",
    "    positionals.append((radius, offset))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Warp Lane Markings to Perspective View\n",
    "\n",
    "Use the inverse transform matrix to warp the lane markings back onto the original perspective image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_lane_to_perspective(img_shape, left_poly, right_poly, M_inv, color=[0, 255, 0]):\n",
    "    \n",
    "    \"\"\"Draws a polygon to represent the lane and warps it to overlay the perspective image.\n",
    "    \n",
    "    Parameters:\n",
    "        • img_shape - shape of the image\n",
    "        • left_poly - coefficients describing the polynomial of the left lane marker\n",
    "        • right_poly - coefficients describing the polynomial of the right lane marker\n",
    "        • M_inv - matrix describing the transformation from birds-eye to perspective view\n",
    "        • color - color of the polygon to be drawn\n",
    "        \n",
    "    Returns:\n",
    "        An image of the polygon of the lane. This can then be overlayed on the actual image\"\"\"\n",
    "    \n",
    "    # generate the points of the best fit lane marking curves\n",
    "    left_x, left_y = gen_curve_points(img_shape, left_poly)\n",
    "    right_x, right_y = gen_curve_points(img_shape, right_poly)\n",
    "    \n",
    "    # create an image, on which to draw the lines\n",
    "    zeros = np.zeros(img_shape[0:2], dtype=np.uint8)\n",
    "    birdseye = np.dstack((zeros, zeros, zeros))\n",
    "\n",
    "    # recast the x and y points into usable format for cv2.fillPoly()\n",
    "    points_left = np.array([np.transpose(np.vstack([left_x, left_y]))])\n",
    "    points_right = np.array([np.flipud(np.transpose(np.vstack([right_x, right_y])))])\n",
    "    points = np.hstack((points_left, points_right))\n",
    "\n",
    "    # draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(birdseye, [points.astype(np.int)], color)\n",
    "\n",
    "    # warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    perspective = cv2.warpPerspective(birdseye, M_inv, (img_shape[1], img_shape[0]))\n",
    "    \n",
    "    return perspective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overlay_detected_lane(img, left_poly, right_poly, M_inv, color=[0, 255, 0]):\n",
    "    \n",
    "    \"\"\"Helper function to overlay the detected lane onto an image\n",
    "    \n",
    "    Parameters:\n",
    "        • img - input image to get the overlay\n",
    "        • left_poly - coefficients describing the polynomial of the left lane marker\n",
    "        • right_poly - coefficients describing the polynomial of the right lane marker\n",
    "        • M_inv - matrix describing the transformation from birds-eye to perspective view\n",
    "        • color - color of the polygon to be drawn\n",
    "        \n",
    "    Returns:\n",
    "        An image with the polygon representing the detected lane overlayed on it\"\"\"\n",
    "    \n",
    "    # create an image with the lane polygon\n",
    "    lane_img = transform_lane_to_perspective(img.shape, left_poly, right_poly, M_inv, color)\n",
    "    \n",
    "    # overlay the lane polygon onto the original image\n",
    "    lane_img = cv2.addWeighted(img, 1, lane_img, 0.3, 0)\n",
    "    \n",
    "    return lane_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def overlay_lane_info(img, radius, offset):\n",
    "    \n",
    "    \"\"\"Helper function to overlay the lane information onto image\n",
    "    \n",
    "    Parameters:\n",
    "        • img - input image of the lane\n",
    "        • radius - radius of the lane curvature\n",
    "        • offset - offset of the car from the center of the lane\n",
    "        \n",
    "    Returns:\n",
    "        An image with the info about the lane overlayed\"\"\"\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text1 = 'Radius of curvature: {:,.0f}m'.format(radius)\n",
    "    text2 = 'Vehicle is {:.2f}m {} of center'.format(abs(offset), offset < 0.0 and 'right' or 'left')\n",
    "    \n",
    "    overlay_img = np.copy(img)\n",
    "    \n",
    "    cv2.putText(overlay_img, text1, (10, 60), font, 2.0, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(overlay_img, text2, (10, 130), font, 2.0, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return overlay_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for birdseye_img, undistort_img, positional, file_base in zip(birdseye_imgs, undistort_imgs, positionals, file_bases):\n",
    "\n",
    "    left_points, right_points, left_windows, right_windows = detect_lane_markings(birdseye_img)\n",
    "\n",
    "    left_poly = fit_curve(left_points)\n",
    "    right_poly = fit_curve(right_points)\n",
    "    \n",
    "    M, M_inv = gen_transform_matrices()\n",
    "    \n",
    "    overlay_img = overlay_detected_lane(undistort_img, left_poly, right_poly, M_inv)\n",
    "    \n",
    "    radius, offset = positional\n",
    "\n",
    "    overlay_img = overlay_lane_info(overlay_img, radius, offset)\n",
    "    \n",
    "    cv2.imwrite('output_images/{}-05-overlay.jpg'.format(file_base), overlay_img)\n",
    "        \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    ax1.set_title('Undistorted Image')\n",
    "    ax1.imshow(cv2.cvtColor(undistort_img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    ax2.set_title('Overlay Lane Image')\n",
    "    ax2.imshow(cv2.cvtColor(overlay_img, cv2.COLOR_BGR2RGB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Pipeline\n",
    "\n",
    "Put all the pieces (after the camera calibration) together into a single pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_lane(img, mtx=None, dist=None):\n",
    "    \n",
    "    \"\"\"Find the lane in the image and overlay the detected lane and information about the position of the car\n",
    "    and the curvature of the lane\n",
    "    \n",
    "    Parameters:\n",
    "        • img - input image\n",
    "        • mtx - camera matrix\n",
    "        • dist - distortion coefficients\n",
    "        \n",
    "     Returns:\n",
    "         The input image with the detected lane and information about the lane overlayed\"\"\"\n",
    "    \n",
    "    overlay_img = np.copy(img)\n",
    "    \n",
    "    if not mtx is None and not dist is None:\n",
    "        # undistort the image\n",
    "        overlay_img = undistort(overlay_img, mtx, dist)\n",
    "        \n",
    "    # threshold the image\n",
    "    working_img = threshold(overlay_img)\n",
    "    \n",
    "    # transform the image to a birds-eye view\n",
    "    working_img = transform_birdseye(working_img)\n",
    "    \n",
    "    # detect the lane markings\n",
    "    left_points, right_points, left_windows, right_windows = detect_lane_markings(working_img)\n",
    "    \n",
    "    # determine the polynomial coefficients for the two lanes\n",
    "    left_poly = fit_curve(left_points)\n",
    "    right_poly = fit_curve(right_points)\n",
    "    \n",
    "    # get the transform matrices\n",
    "    M, M_inv = gen_transform_matrices()\n",
    "\n",
    "    # overlay the detected lane onto the original undistorted image\n",
    "    overlay_img = overlay_detected_lane(overlay_img, left_poly, right_poly, M_inv)\n",
    "    \n",
    "    # calculate lane curvature\n",
    "    radius = calc_lane_curvature(working_img.shape, left_poly, right_poly)\n",
    "    \n",
    "    # calculate offset of the vehicle from the center of the lane\n",
    "    offset = calc_lane_center_offset(working_img.shape, left_poly, right_poly)\n",
    "    \n",
    "    # overlay the lane information (curvature radius and offset from center)\n",
    "    overlay_img = overlay_lane_info(overlay_img, radius, offset)\n",
    "    \n",
    "    return overlay_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename, file_base in zip(glob.glob('test_images/*.jpg'), file_bases):\n",
    "    \n",
    "    orig_img = cv2.imread(filename)\n",
    "    overlay_img = find_lane(orig_img, mtx, dist)\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    ax1.set_title('Original Image')\n",
    "    ax1.imshow(cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    ax2.set_title('Overlay Lane Image')\n",
    "    ax2.imshow(cv2.cvtColor(overlay_img, cv2.COLOR_BGR2RGB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Test on Videos\n",
    "\n",
    "Let's try the pipeline on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. project_video.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project_output = 'project_output.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "project_clip = clip1.fl_image(lambda x: find_lane(x, mtx, dist)) #NOTE: this function expects color images!!\n",
    "%time project_clip.write_videofile(project_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(project_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. challenge_video.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "challenge_output = 'challenge_output.mp4'\n",
    "clip2 = VideoFileClip(\"challenge_video.mp4\")\n",
    "challenge_clip = clip2.fl_image(lambda x: find_lane(x, mtx, dist)) #NOTE: this function expects color images!!\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. harder_challenge_video.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "harder_challenge_output = 'harder_challenge_output.mp4'\n",
    "clip3 = VideoFileClip(\"harder_challenge_video.mp4\")\n",
    "harder_challenge_clip = clip3.fl_image(lambda x: find_lane(x, mtx, dist)) #NOTE: this function expects color images!!\n",
    "%time harder_challenge_clip.write_videofile(harder_challenge_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(harder_challenge_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
